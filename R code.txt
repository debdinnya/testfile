#########################################################
#########################################################
######################################
###### Incident Volume Forecast ######
######################################
#########################################################
#########################################################

######Changes#######################
# Aug 5, 2019:
#  1# Removed line 505: #WC_subset_dev1 <- WC_subset_dev1[which(WC_subset_dev1$YR_WK >= WC_yrwk),] # not required - Commented out as of Aug 5, 2019
#  2# Removed Line 300: #model <- nnetar(y=trn$nDep,p=4,P=,Size=70,repeats=50,lambda=0,xreg=trn[,xregvar])

setwd("C:/DataScience/July-2020") # setting a working directory
#setwd("C:/Datascience/Dryrun/Aug08")

# Load required libraries
library("readxl")
library(sqldf)
library(lubridate)
library (ggplot2) #Forecast function is dependent on ggplot2
library(forecast)

# Get data 
#library(odbc) #not required here as CSV file will be read
Indat_c<-read.csv("Incident.csv", header=TRUE,sep=",")
#Indat_c<-read.csv("in_Test.csv", header=TRUE,sep=",") # for small set of data

str(Indat_c) # Structure of dataframe

#renaming column names
colnames(Indat_c)[1]<-"Creation_Date"
colnames(Indat_c)[6]<-"Solution_Area"
colnames(Indat_c)[5]<-"Group_Name"
colnames(Indat_c)[3]<-"Incident_Type"
colnames(Indat_c)[2]<-"TicketID"
colnames(Indat_c)[7]<-"Creation_YRWk"


# Change dateformat to ISO date
#Indat_c$Creation_Date = as.Date(Indat_c$Creation_Date,format = "%d/%m/%Y")


# Read event data
# Event = read_excel("Event.xlsx")
Event <- read.csv("Event.csv", header=TRUE,sep=",")

#Event$Creation_Date = as.Date(Event$Creation_Date,format = "%d/%m/%Y")

# Group IBM-CMB-BPT-SL1 is incorrect, change it to IBM-CMB-BPT-SL2
Indat_c$Group_Name[Indat_c$Group_Name == "IBM-CMB-BPT-SL1"] <- "IBM-CMB-BPT-SL2"

#Merge
Indat_c <- merge(Indat_c,Event,by=c("Creation_Date","Solution_Area","Group_Name"),all.x=TRUE)

# check for null values
s<-sqldf('select * from Indat_c where Factor is null')
View(s)
#Indat_c <- merge(Indat_c,Event,by="Creation_Date",all.x=TRUE)

tail(Indat_c)

Indat_c$Factor[is.na(Indat_c$Factor)] <- 'L'

Indat_c <- Indat_c[which(Indat_c$Solution_Area == "OBP" | Indat_c$Solution_Area == "P&FU"),]
Indat_c <- Indat_c[which(Indat_c$Incident_Type != ""),]
s<-sqldf('select * from Indat_c where Factor is null')

#Indat_c$OBP_Major_Change <- ifelse(is.na(Indat_c$OBP_Major_Change),0,Indat_c$OBP_Major_Change)
#Indat_c$PNFU_Major_Change <- ifelse(is.na(Indat_c$PNFU_Major_Change),0,Indat_c$PNFU_Major_Change)
#Indat_c$Lean_Period <- ifelse(is.na(Indat_c$Lean_Period),0,Indat_c$Lean_Period)
#Indat_c$Year_End <- ifelse(is.na(Indat_c$Year_End),0,Indat_c$Year_End)


#Indat_c$Creation_Date = as.Date(Indat_c$Creation_Date,format = "%m/%d/%y")
#Indat_c$Creation_YRWk <- as.integer(paste(substr(year(Indat_c$Creation_Date),3,4),sprintf("%02d", week(Indat_c$Creation_Date)), sep=""))
Indat_c$Creation_MN <- as.integer(month(Indat_c$Creation_Date))


CR_YRWK_ALL <- sqldf('select distinct Creation_YRWk from Indat_c')
CR_SA_ALL <- sqldf('select distinct Solution_Area from Indat_c')
CR_GN_ALL <- sqldf('select distinct Group_Name from Indat_c')
View(CR_GN_ALL)
CR_IT_ALL <- sqldf('select distinct Incident_Type from Indat_c')
CR_Score_All <- sqldf('select distinct Factor from Indat_c')

ALL1 <- sqldf('select * from CR_SA_ALL,CR_GN_ALL,CR_IT_ALL,CR_YRWK_ALL, CR_Score_All')


tsdat_prep1 <- sqldf('select A.Solution_Area,A.Group_Name,A.Incident_Type,A.Creation_YRWk,A.Factor, max(A.Creation_MN) as Creation_MN,Count(distinct A.TicketID) as WK_CT
                     
                     from Indat_c A
                     
                     group by A.Solution_Area,A.Group_Name,A.Incident_Type,A.Creation_YRWk
                     order by A.Solution_Area,A.Group_Name,A.Incident_Type,A.Creation_YRWk')




tsdat_prep2 <- sqldf('select A.Solution_Area,A.Group_Name,A.Incident_Type,A.Creation_YRWk,Factor,max(A.Creation_MN) Creation_MN,max(Score) Score
                     
                     from Indat_c A
                     
                     group by A.Solution_Area,A.Group_Name,A.Incident_Type,A.Creation_YRWk,Factor
                     order by A.Solution_Area,A.Group_Name,A.Incident_Type,A.Creation_YRWk,Factor')




#transpose
tsdat_prep_transposed1<-reshape(tsdat_prep2, direction = "wide", idvar = c("Solution_Area","Group_Name","Incident_Type","Creation_YRWk","Creation_MN"), timevar = "Factor")

# without Creation_MN
tsdat_prep1 <- sqldf('select A.Solution_Area,A.Group_Name,A.Incident_Type,A.Creation_YRWk,Count(distinct TicketID) as WK_CT
                     
                     from Indat_c A
                     
                     group by A.Solution_Area,A.Group_Name,A.Incident_Type,A.Creation_YRWk
                     order by A.Solution_Area,A.Group_Name,A.Incident_Type,A.Creation_YRWk')


tsdat_prep_transposed <- merge(tsdat_prep1,tsdat_prep_transposed1,by=c("Solution_Area","Group_Name","Incident_Type","Creation_YRWk"),all.x=TRUE)

n <- ncol(tsdat_prep_transposed)

z <-colnames(tsdat_prep_transposed[(ncol(tsdat_prep_transposed)-3):ncol(tsdat_prep_transposed)]) 
View(z)

v1 <- c(z) #putting into a vector V
View(v1)
tsdat <- tsdat_prep_transposed



tsdat_all <- sqldf('select A.Solution_Area,A.Group_Name,A.Incident_Type,A.Creation_YRWk,
                   B.*
                   
                   from ALL1 A left outer join tsdat B
                   on A.Creation_YRWk = B.Creation_YRWk
                   and A.Solution_Area = B.Solution_Area
                   and A.Group_Name = B.Group_Name
                   and A.Incident_Type = B.Incident_Type
                   order by A.Solution_Area,A.Group_Name,A.Incident_Type,A.Creation_YRWk')

tsdat_all <- tsdat_all[ -c(5,6,7,8) ]

tsdat_all$nDep <- ifelse(is.na(tsdat_all$WK_CT),0,tsdat_all$WK_CT)

MN_CT <- sqldf('select Solution_Area,Group_Name,Incident_Type,Creation_MN,Count(TicketID) as MN_CT
               from Indat_c
               group by Solution_Area,Group_Name,Incident_Type,Creation_MN')

TOT_CT <- sqldf('select Solution_Area,Group_Name,Incident_Type,Count(TicketID) as TOT_CT
                from Indat_c
                group by Solution_Area,Group_Name,Incident_Type')

tsdat_fin <- sqldf('select A.* , B.MN_CT, C.TOT_CT
                   
                   from tsdat_all A
                   
                   left outer join MN_CT B on
                   (
                   A.Solution_Area = B.Solution_Area
                   and A.Group_Name = B.Group_Name
                   and A.Incident_Type = B.Incident_Type
                   and A.Creation_MN = B.Creation_MN
                   )
                   
                   left outer join TOT_CT C on
                   (
                   B.Solution_Area = C.Solution_Area
                   and B.Group_Name = C.Group_Name
                   and B.Incident_Type = C.Incident_Type
                   )
                   
                   group by A.Solution_Area,A.Group_Name,A.Incident_Type,A.Creation_YRWk
                   order by A.Solution_Area,A.Group_Name,A.Incident_Type,A.Creation_YRWk')


#####################
#dropping 52 and 53 weeks

tsdat_fin <- sqldf('select * from tsdat_fin where Creation_YRWk != 201652 
                   and Creation_YRWk != 201653 
                   and Creation_YRWk != 201752 
                   and Creation_YRWk != 201753 
                   and Creation_YRWk != 201852 
                   and Creation_YRWk != 201853
                   and Creation_YRWk != 201952
                   and Creation_YRWk != 201953
                   
                   ')




####################

#####################
# Seasonal index
####################

tsdat_fin$SeasIdx = tsdat_fin$MN_CT/tsdat_fin$TOT_CT
#tsdat_fin$Creation_MN

d <- sqldf( "select distinct Creation_MN, SeasIdx from tsdat_fin")


dropvars <- names(tsdat_fin) %in% c("Creation_MN","MN_CT","TOT_CT","WK_CT")
tsdat_fin <- tsdat_fin[!dropvars]
#tsdat_fin$OBP_Major_Change[is.na(tsdat_fin$OBP_Major_Change)] <- 0
#tsdat_fin$PNFU_Major_Change[is.na(tsdat_fin$PNFU_Major_Change)] <- 0
#tsdat_fin$Lean_Period[is.na(tsdat_fin$Lean_Period)] <- 0
#tsdat_fin$Year_End[is.na(tsdat_fin$Year_End)] <- 0
tsdat_fin$Score.L[is.na(tsdat_fin$Score.L)] <- 0
tsdat_fin$Score.Y[is.na(tsdat_fin$Score.Y)] <- 0
tsdat_fin$Score.S1[is.na(tsdat_fin$Score.S1)] <- 0

tsdat_fin$Score.S2[is.na(tsdat_fin$Score.S2)] <- 0

tsdat_fin$SeasIdx[is.na(tsdat_fin$SeasIdx)] <- 0.08


tsdat_fin$trend <- as.integer(ave(tsdat_fin$Creation_YRWk, tsdat_fin$Solution_Area,tsdat_fin$Group_Name,tsdat_fin$Incident_Type, FUN = seq_along))


######################

#write.csv(tsdat_fin,"tsdat_fin_Pranab.csv",row.names=FALSE)
#tsdat_dev
#write.csv(tsdat_dev,"tsdat_dev_Pranab.csv",row.names=FALSE)
######################

# get system date
dt <- Sys.Date() 
View(dt)
yr_wk <- paste(substr(year(Sys.Date()),1,4),sprintf("%02d", isoweek(Sys.Date())), sep="")
class(yr_wk)
yr_wk1 <- as.numeric(yr_wk) # forecast start week
yr_wk1<-yr_wk1
yr_wk2 <- yr_wk1 + 7   #forecast end week 

yw_fm<-yr_wk1 -100
yw_to<-yr_wk2 -100
yr_wk_training <- yr_wk1 - 12 #testing weeks
yr_wk_training_start <- yr_wk_training-294 #getting 148 weeks for training

tsdat_dev = tsdat_fin[which(tsdat_fin$Creation_YRWk < yr_wk_training & tsdat_fin$Creation_YRWk >=yr_wk_training_start),]

tsdat_val = tsdat_fin[which(tsdat_fin$Creation_YRWk >= yr_wk_training & tsdat_fin$Creation_YRWk < yr_wk1),]

tsdat_prd = tsdat_dev[which(tsdat_dev$Creation_YRWk >= yw_fm & tsdat_dev$Creation_YRWk <= yw_to),]
tsdat_prd$Creation_YRWk = tsdat_prd$Creation_YRWk + 100
tsdat_prd$trend = tsdat_prd$trend + 53

# just checking distinct yearwk for each dataset#
#cc<-sqldf('select distinct Creation_YRWK from tsdat_prd')

library(forecast)

xregvar <- c("trend","SeasIdx")

# addinng the factors dynamically 
xregvar <- c(xregvar, v1)
#xregvar <- c("trend","SeasIdx")s
View(xregvar)



for (s in 1:nrow(CR_SA_ALL)){
  for (g in 1:nrow(CR_GN_ALL)){
    for (i in 1:nrow(CR_IT_ALL)){
      satmp="OBP"
      gntmp="IBM-CMB-BPT-SL2"
      ittmp="Infrastructure Event"
      satmp <- CR_SA_ALL[s,]
      gntmp <- CR_GN_ALL[g,]
      ittmp <- CR_IT_ALL[i,]
      
      print(paste("Running for Solution Area: ",satmp," s=",s," Group Name: ",gntmp," g=",g," Incident Type:",ittmp," i=",i,sep=""))
      
      trn <- tsdat_dev[which(tsdat_dev$Solution_Area == satmp & tsdat_dev$Group_Name == gntmp & tsdat_dev$Incident_Type == ittmp),]
      trn$samp = "trn"
      tst <- tsdat_val[which(tsdat_val$Solution_Area == satmp & tsdat_val$Group_Name == gntmp & tsdat_val$Incident_Type == ittmp),]
      tst$samp = "tst"
      prd <- tsdat_prd[which(tsdat_prd$Solution_Area == satmp & tsdat_prd$Group_Name == gntmp & tsdat_prd$Incident_Type == ittmp),]
      prd$samp = "prd"
      
      for(j in 70:85){#5:50
        for(k in 4:13){#4:13
          
          trn$nDep <- ifelse(trn$nDep == 0,0.0001,trn$nDep)
          
          model <- nnetar(y=trn$nDep,p=k,P=,Size=j,repeats=50,lambda=0,xreg=trn[,xregvar])
          #commented on Aug 5, 2019
          #model <- nnetar(y=trn$nDep,p=4,P=,Size=70,repeats=50,lambda=0,xreg=trn[,xregvar])
          
          pred_trn <- as.data.frame(forecast(model,h=146,xreg = trn[,xregvar]))
          pred_tst <- as.data.frame(forecast(model,h=13,xreg = tst[,xregvar]))
          pred_prd <- as.data.frame(forecast(model,h=8,xreg = prd[,xregvar]))
          
          
          tot <- rbind(trn,tst,prd)
          pred <- rbind(pred_trn,pred_tst,pred_prd)
          names(pred) <- paste0("j",j,"_k",k)
          if(j==70 & k==4){pred_tot <- cbind(tot,pred)}
          else{pred_tot <- cbind(pred_tot,pred)}
          
          lhs  <- paste("pred_tot$ape_j",j,"_k",k,sep="")
          rhs  <- paste("abs(pred_tot$nDep-pred_tot$j",j,"_k",k,")/pred_tot$nDep",sep="")
          eq   <- paste(paste(lhs, rhs, sep="<-"), collapse=";")
          eval(parse(text=eq))
          
          tblnm <- paste("smry",sep="")
          expr <- paste("sqldf('select samp,avg(ape_j",j,"_k",k,") as mape_j",j,"_k",k," from pred_tot group by samp')",sep="")
          eq2   <- paste(paste(tblnm, expr, sep="<-"), collapse=";")
          eval(parse(text=eq2))
          
          smry$Size = j
          smry$Lag = k
          smry1 <- smry[which(smry$samp == "trn"),]
          dropvars <- names(smry1) %in% c("samp")
          smry1 <- smry1[!dropvars]
          names(smry1)[1] <- "trn_mape"
          smry2 <- smry[which(smry$samp == "tst"),]
          dropvars <- names(smry2) %in% c("samp")
          smry2 <- smry2[!dropvars]
          names(smry2)[1] <- "tst_mape"
          smry3 <- merge(smry1,smry2, by = c("Lag","Size"))
          
          if(s==1 & g==1 & i==1 & j==70 & k==4){
            smry3$Solution_Area = satmp
            smry3$Group_Name = gntmp
            smry3$Incident_Type = ittmp
            smry_fin <- smry3
          }
          else{
            smry3$Solution_Area = satmp
            smry3$Group_Name = gntmp
            smry3$Incident_Type = ittmp
            smry_fin <- rbind(smry_fin,smry3)
          }
          
        }
      }
      
      trn_min <- sqldf('select Size,Lag,trn_mape as mape from smry_fin order by trn_mape limit 1')
      tst_min <- sqldf('select Size,Lag,tst_mape as mape from smry_fin order by tst_mape limit 1')
      mape_min <- rbind(trn_min,tst_min)
      mape_min <- mape_min[!duplicated(mape_min[,c('Lag', 'Size')]),]
      
      for(a in 1:nrow(mape_min)){
        j = mape_min$Size[a]
        k = mape_min$Lag[a]
        
        tblnm <- paste("tmp_tbl",sep="")
        expr <- paste("as.data.frame(pred_tot$j",j,"_k",k,")",sep="")
        eq2   <- paste(paste(tblnm, expr, sep="<-"), collapse=";")
        eval(parse(text=eq2))
        names(tmp_tbl) <- paste("col",a,sep="")
        
        if(a==1){pred_fin<-tmp_tbl}
        else{pred_fin<-cbind(pred_fin,tmp_tbl)}
      }
      pred_fin$pred = rowMeans(pred_fin)
      pred_fin_tot_all = cbind(pred_tot,pred_fin$pred)
      colnames(pred_fin_tot_all)[ncol(pred_fin_tot_all)] <- "pred"
      pred_fin_tot_all$nDep = ifelse(pred_fin_tot_all$samp == 'prd',0,pred_fin_tot_all$nDep)
      
      filename = paste0("pred_NN_",satmp,"_",gntmp,"_",ittmp,".csv",sep="")
      write.csv(pred_fin_tot_all,filename,row.names=FALSE)
      
      pred_fin_tot = cbind(pred_tot[,1:12],pred_fin$pred)
      colnames(pred_fin_tot)[13] <- "pred"
      pred_fin_tot$nDep = ifelse(pred_fin_tot$samp == 'prd',0,pred_fin_tot$nDep)
      if (s==1 & g==1 & i == 1){pred_tot_fin_fin <- pred_fin_tot}
      else {pred_tot_fin_fin <- rbind(pred_tot_fin_fin,pred_fin_tot)}
      
    }
  }
}

meas <- sqldf('select Solution_Area,Group_Name,Incident_Type,avg(nDep) as mean,stdev(nDep) as sd
              from pred_tot_fin_fin
              where samp = "trn"
              group by Solution_Area,Group_Name,Incident_Type')

pred_tot_fin_fin1 <- sqldf('select A.*,B.mean,B.sd
                           from pred_tot_fin_fin A left outer join meas B on
                           A.Solution_Area=B.Solution_Area
                           and A.Group_Name=B.Group_Name
                           and A.Incident_Type=B.Incident_Type
                           ')
pred_tot_fin_fin1$pred1 = ifelse(pred_tot_fin_fin1$pred > (pred_tot_fin_fin1$mean + 6*pred_tot_fin_fin1$sd),(pred_tot_fin_fin1$mean + 1.5*pred_tot_fin_fin1$sd),pred_tot_fin_fin1$pred)

pred_tot_fin_fin1$pred1 = round(pred_tot_fin_fin1$pred1, digits = 0)

dropvars <- names(pred_tot_fin_fin1) %in% c("mean","sd","pred")
pred_tot_fin_fin1 <- pred_tot_fin_fin1[!dropvars]
colnames(pred_tot_fin_fin1)[13] <- "pred"

write.csv(pred_tot_fin_fin1,"pred_tot_csv_20.csv",row.names=FALSE)
write.csv(smry_fin,"smry_fin.csv",row.names=FALSE)

outdat <- pred_tot_fin_fin1[which(pred_tot_fin_fin1$samp == 'prd'),]

# need to drop the factor dynamically
dropvars <- names(outdat) %in% c("SeasIdx","nDep","samp","trend", v1)
outdat <- outdat[!dropvars]
outdat1 <- reshape(outdat,
                   timevar = "Creation_YRWk",idvar=c("Solution_Area","Group_Name","Incident_Type"),
                   direction = "wide",v.names="pred")
#colnames(outdat1)[4:11] <- c("Forecast_May2018","Forecast_June2018","Forecast_July2018")
write.csv(outdat1,"Outdat_20_1.csv",row.names=FALSE)
write.csv(outdat,"Outdat2_20_1.csv",row.names=FALSE)


###############################################################

#Rerun of model for on adhoc basis
###############################################################

meas_1 <- sqldf('select Solution_Area,Group_Name,Incident_Type,avg(nDep) as mean,stdev(nDep) as sd
              from pred_tot_fin_fin
              where samp = "trn"
              group by Solution_Area,Group_Name,Incident_Type')

pred_tot_fin_fin1_1 <- sqldf('select A.*,B.mean,B.sd
                           from pred_tot_fin_fin A left outer join meas B on
                           A.Solution_Area=B.Solution_Area
                           and A.Group_Name=B.Group_Name
                           and A.Incident_Type=B.Incident_Type
                           ')
pred_tot_fin_fin1_1$pred1 = ifelse(pred_tot_fin_fin1_1$pred > (pred_tot_fin_fin1_1$mean + 6*pred_tot_fin_fin1_1$sd),(pred_tot_fin_fin1_1$mean + 1.5*pred_tot_fin_fin1_1$sd),pred_tot_fin_fin1_1$pred)

pred_tot_fin_fin1_1$pred1 = round(pred_tot_fin_fin1_1$pred1, digits = 0)

dropvars_2<- names(pred_tot_fin_fin1_1) %in% c("mean","sd","pred")
pred_tot_fin_fin1_1 <- pred_tot_fin_fin1_1[!dropvars_2]
colnames(pred_tot_fin_fin1_1)[13] <- "pred"


pred_tot_fin_fin1_1<-read.csv("pred_tot_csv_20.csv", header=TRUE,sep=",")

outdat_1 <- pred_tot_fin_fin1_1[which(pred_tot_fin_fin1_1$samp == 'prd'),]

# need to drop the factor dynamically
dropvars_1 <- names(outdat_1) %in% c("Model_Run_Date","SeasIdx","nDep","samp","trend", v1)
outdat_1 <- outdat_1[!dropvars_1]

write.csv(outdat_1,"Outdat2_20_1_1.csv",row.names=FALSE)


##############################################################


###############################################################
##############################################################
#######
#######       End of Incident Prediction
#######
##############################################################
##############################################################

###############################################################
##############################################################
#######
#######       Text Mining - Word cloud
#######
##############################################################
##############################################################

library(RODBC)
library(dplyr)
library(caret)
library(tm)
library(wordcloud)
library(readxl)
library(lubridate)

library(caret)
library(tm)
library(wordcloud)
library(readxl)

# Read input data
WC <- read_excel("Incident_WC_1.xls")
#WC <-read.csv("Incident_WC_1.csv", header=TRUE,sep=",")

# Select only the required columns
WC_subset <- subset(WC, select = c("SMRRY","SERVICE_CI", "YR_WK","SOL_AREA","GROUP_NAME","INCIDENT_TYPE","CREATION_DAT"))

#dt <- Sys.Date()
#yr_wk <- as.numeric(paste(substr(year(Sys.Date()),3,4),sprintf("%02d", isoweek(Sys.Date())), sep=""))
#class(yr_wk)

## First Solution Area

## Infrastructure Restoration
WC_subset_dev1 <- WC_subset[which(WC_subset$SOL_AREA == CR_SA_ALL[1,] &   WC_subset$INCIDENT_TYPE == "Infrastructure Restoration" ),]

#max_weeknr <- sqldf('select max(YR_WK) from WC')
#View(max_weeknr)
#WC_yrwk <- max_weeknr - 4 
#WC_subset_dev1 <- WC_subset_dev1[which(WC_subset_dev1$YR_WK >= WC_yrwk),] # not required - Commented out as of Aug 5, 2019

library(tm)
corpus <- Corpus(VectorSource(WC_subset_dev1$SMRRY)) 
corpus <- tm_map(corpus, content_transformer(tolower))
#corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, removeWords, c("please","will","hej")) 
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
tdm<- TermDocumentMatrix(corpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing = TRUE)
d <- data.frame(word = names(v),freq=v)
d$sa <- 'OBP'
d$incident_type <-'IR'


## Infrastructure Event
WC_subset_dev2 <- WC_subset[which(WC_subset$SOL_AREA == CR_SA_ALL[1,] &   WC_subset$INCIDENT_TYPE == "Infrastructure Event" ),]
#WC_subset_dev2 <- WC_subset_dev2[which(WC_subset_dev2$YR_WK >= WC_yrwk),]

corpus1 <- Corpus(VectorSource(WC_subset_dev2$SMRRY))
corpus1 <- tm_map(corpus1, content_transformer(tolower))
#corpus <- tm_map(corpus, removeNumbers)
corpus1 <- tm_map(corpus1, removeWords, stopwords("english"))
corpus1 <- tm_map(corpus1, removeWords, c("please","will","hej")) 
corpus1 <- tm_map(corpus1, removePunctuation)
corpus1 <- tm_map(corpus1, stripWhitespace)
tdm1<- TermDocumentMatrix(corpus1)
m1 <- as.matrix(tdm1)
v1 <- sort(rowSums(m1),decreasing = TRUE)
d1 <- data.frame(word = names(v1),freq=v1)
d1$sa <- 'OBP'
d1$incident_type <-'IE'
max_weeknr <- sqldf('select max(YR_WK) from WC')
#View(max_weeknr)

## User Service Request
WC_subset_dev3 <- WC_subset[which(WC_subset$SOL_AREA == CR_SA_ALL[1,] &   WC_subset$INCIDENT_TYPE == "User Service Request" ),]

corpus2 <- Corpus(VectorSource(WC_subset_dev3$SMRRY))
corpus2 <- tm_map(corpus2, content_transformer(tolower))
#corpus <- tm_map(corpus, removeNumbers)
corpus2 <- tm_map(corpus2, removeWords, stopwords("english"))
corpus2 <- tm_map(corpus2, removeWords, c("please","will","hej")) 
corpus2 <- tm_map(corpus2, removePunctuation)
corpus2 <- tm_map(corpus2, stripWhitespace)
tdm2<- TermDocumentMatrix(corpus2)
m2 <- as.matrix(tdm2)
v2 <- sort(rowSums(m2),decreasing = TRUE)
d2 <- data.frame(word = names(v2),freq=v2)
d2$sa <- 'OBP'
d2$incident_type <-'USR'


## User Service Restoration
WC_subset_dev4 <- WC_subset[which(WC_subset$SOL_AREA == CR_SA_ALL[1,] &   WC_subset$INCIDENT_TYPE == "User Service Restoration" ),]

corpus3 <- Corpus(VectorSource(WC_subset_dev4$SMRRY))
corpus3 <- tm_map(corpus3, content_transformer(tolower))
#corpus <- tm_map(corpus, removeNumbers)
corpus3 <- tm_map(corpus3, removeWords, stopwords("english"))
corpus3 <- tm_map(corpus3, removeWords, c("please","will","hej")) 
corpus3 <- tm_map(corpus3, removePunctuation)
corpus3 <- tm_map(corpus3, stripWhitespace)
tdm3<- TermDocumentMatrix(corpus3)
m3 <- as.matrix(tdm3)
v3 <- sort(rowSums(m3),decreasing = TRUE)
d3 <- data.frame(word = names(v3),freq=v3)
d3$sa <- 'OBP'
d3$incident_type <-'user service restoration'


## next Solution Area

## user service restoration
WC_subset_dev5 <- WC_subset[which(WC_subset$SOL_AREA == CR_SA_ALL[2,] &   WC_subset$INCIDENT_TYPE == "User Service Restoration" ),]

corpus4 <- Corpus(VectorSource(WC_subset_dev5$SMRRY))
corpus4 <- tm_map(corpus4, content_transformer(tolower))
#corpus <- tm_map(corpus, removeNumbers)
corpus4 <- tm_map(corpus4, removeWords, stopwords("english"))
corpus4 <- tm_map(corpus4, removeWords, c("please","will","hej")) 
corpus4 <- tm_map(corpus4, removePunctuation)
corpus4 <- tm_map(corpus4, stripWhitespace)
tdm4 <- TermDocumentMatrix(corpus4)
m4 <- as.matrix(tdm4)
v4 <- sort(rowSums(m4),decreasing = TRUE)
d4 <- data.frame(word = names(v4),freq=v4)
d4$sa <- 'PFU'
d4$incident_type <-'user service restoration'

## Infrastructure Restoration
WC_subset_dev6 <- WC_subset[which(WC_subset$SOL_AREA == CR_SA_ALL[2,] &   WC_subset$INCIDENT_TYPE == "Infrastructure Restoration" ),]

corpus5 <- Corpus(VectorSource(WC_subset_dev6$SMRRY))
corpus5 <- tm_map(corpus5, content_transformer(tolower))
#corpus <- tm_map(corpus, removeNumbers)
corpus5 <- tm_map(corpus5, removeWords, stopwords("english"))
corpus5 <- tm_map(corpus5, removeWords, c("please","will","hej")) 
corpus5 <- tm_map(corpus5, removePunctuation)
corpus5 <- tm_map(corpus5, stripWhitespace)
tdm5 <- TermDocumentMatrix(corpus5)
m5 <- as.matrix(tdm5)
v5 <- sort(rowSums(m5),decreasing = TRUE)
d5 <- data.frame(word = names(v5),freq=v5)
d5$sa <- 'PFU'
d5$incident_type <-'IR'

# User Service Request
WC_subset_dev7 <- WC_subset[which(WC_subset$SOL_AREA == CR_SA_ALL[2,] &   WC_subset$INCIDENT_TYPE == "User Service Request" ),]

corpus6 <- Corpus(VectorSource(WC_subset_dev7$SMRRY))
corpus6 <- tm_map(corpus6, content_transformer(tolower))
#corpus <- tm_map(corpus, removeNumbers)
corpus6 <- tm_map(corpus6, removeWords, stopwords("english"))
corpus6 <- tm_map(corpus6, removeWords, c("please","will","hej")) 
corpus6 <- tm_map(corpus6, removePunctuation)
corpus6 <- tm_map(corpus6, stripWhitespace)
tdm6 <- TermDocumentMatrix(corpus6)
m6 <- as.matrix(tdm6)
v6 <- sort(rowSums(m6),decreasing = TRUE)
d6 <- data.frame(word = names(v6),freq=v6)
d6$sa <- 'PFU'
d6$incident_type <-'USR'

# Infrastructure Event
WC_subset_dev8 <- WC_subset[which(WC_subset$SOL_AREA == CR_SA_ALL[2,] &   WC_subset$INCIDENT_TYPE == "Infrastructure Event" ),]

corpus7 <- Corpus(VectorSource(WC_subset_dev8$SMRRY))
corpus7 <- tm_map(corpus7, content_transformer(tolower))
#corpus <- tm_map(corpus, removeNumbers) # removing numbers are not required as of now
corpus7 <- tm_map(corpus7, removeWords, stopwords("english"))
corpus7 <- tm_map(corpus7, removeWords, c("please","will","hej")) 
corpus7 <- tm_map(corpus7, removePunctuation)
corpus7 <- tm_map(corpus7, stripWhitespace)
tdm7 <- TermDocumentMatrix(corpus7)
m7 <- as.matrix(tdm7)
v7 <- sort(rowSums(m7),decreasing = TRUE)
d7 <- data.frame(word = names(v7),freq=v7)
d7$sa <- 'PFU'
d7$incident_type <-'IE'


x<-rbind(d,d1,d2,d3,d4,d5,d6,d7)
write.csv(x,"x.csv",row.names=FALSE)


###############################################################
##############################################################
#######
#######      End of Text Mining - Word cloud
#######
##############################################################
##############################################################

###############################################################
##############################################################
#######
#######      Start of Resolve Time
#######
##############################################################
##############################################################



############################################################
############# IBM Bucket resultion time ####################
#############                           ####################
############################################################


#Read Excel IBM_Bucket_SolArea.xlsx

res_time <- read_excel("IBM_Bucket_SolArea.xlsx")
str(res_time)

# convert to chanracters for combined field
res_time$`Incident Reported Date Time`<- as.character(res_time$`Incident Reported Date Time`)
res_time$`Target Assigned date/time`<- as.character(res_time$`Target Assigned date/time`)
res_time$`Target Response date/time` <- as.character(res_time$`Target Response date/time`)

# Combination for uniquely indentify the IBM only resolve time. Paste function is used for concatenation. 
res_time$combined <- paste(res_time$`Incident Number`,res_time$`Incident Reported Date Time`,res_time$`Target Response date/time`,res_time$`Group Assigned`, sep="")

# convert senconds to hours
res_time$res_time_hour <- res_time$`SLT Elapsed time`/ (60*60)

#convert to days
res_time$res_time_day <- res_time$`SLT Elapsed time`/ (24*60*60)

# round off to 2 digits
res_time$res_time_hour <- round(res_time$res_time_hour, 2)
res_time$res_time_day <- round(res_time$res_time_day, 2)
str(res_time)

#rename the columns to correct names
colnames(res_time)[which(names(res_time) == "Solution Area")] <- "Solution_Area" 
colnames(res_time)[which(names(res_time) == "Group Assigned")] <- "Group_Assigned" 
colnames(res_time)[which(names(res_time) == "Group_Assigned ")] <- "Group_Assigned" 

# filter only IBM-CMB-BPT-SL2 and IBM-CMB-BRS-SL2
res_time <- res_time[which(res_time$Group_Assigned == CR_GN_ALL[1,1] | res_time$Group_Assigned == CR_GN_ALL[2,1]),]

# Prepare dataframe for Resolve Time in days
res_time_prep <- sqldf('select Priority,Solution_Area, Group_Assigned, combined, sum(res_time_day) as res_time
                    
                    from res_time
                    
                    group by  Priority,Solution_Area, Group_Assigned, combined
                    order by  Priority,Solution_Area, Group_Assigned, combined')

res_time_prep <- res_time_prep[order(res_time_prep$res_time),]

# for analysis, write to a file
write.csv(res_time_prep,"res_time_prep.csv",row.names=FALSE)

# Prepare dataframe for Resolve Time in hours
res_hour_prep <- sqldf('select Priority,Solution_Area, Group_Assigned, combined, sum(res_time_hour) as res_time
                    
                       from res_time
                       
                       group by  Priority,Solution_Area, Group_Assigned, combined
                       order by  Priority,Solution_Area, Group_Assigned, combined')

res_hour_prep <- res_hour_prep[order(res_hour_prep$res_time),]

# Preprocess res_time_prep dataframe (Resolve time in Days)
res_time_prep <- res_time_prep[!(is.na(res_time_prep$res_time)),]  # drop null values
res_time_prep <- res_time_prep[which(res_time_prep$res_time < 10),] # Exclude all incidents more than 10 days
res_time_prep$Solution_Area[res_time_prep$Solution_Area == 'OTH'] = 'OBP' # Aug 6, 2019 - added the line to convert 'OTH' to 'OBP'
res_time_prep <- res_time_prep[!(is.na(res_time_prep$Solution_Area)),]  # Aug 6, 2019 - Drop NA solution area'

str(res_time_prep$Group_Assigned)
str(CR_GN_ALL$Group_Name)

#levels
levels(CR_GN_ALL[g,1])
levels(res_time_prep$Group_Assigned)

# drop unnecessary levels in factors
CR_GN_ALL$Group_Name <- factor(CR_GN_ALL$Group_Name)
CR_GN_ALL$Group_Name 

CR_SA_ALL$Solution_Area <- factor(CR_SA_ALL$Solution_Area)
CR_SA_ALL$Solution_Area

res_time_prep$Solution_Area<- factor(res_time_prep$Solution_Area)
levels(res_time_prep$Solution_Area)
res_time_prep$Group_Assigned<- factor(res_time_prep$Group_Assigned)
levels(res_time_prep$Group_Assigned)

# Unique priority
res_time_prep <- res_time_prep[which(res_time_prep$Priority != 	"Critical"),]
CR_PR_ALL <- sqldf('select distinct Priority from res_time_prep')

#drop unnecessary levels
levels(CR_PR_ALL$Priority)
CR_PR_ALL$Priority <- factor(CR_PR_ALL$Priority)
levels(res_time_prep$Priority)
res_time_prep$Priority <- factor(res_time_prep$Priority)

# for loop to calculate mean resolution time
for (p in 1:nrow(CR_PR_ALL)){
  for (s in 1:nrow(CR_SA_ALL)){
    for (g in 1:nrow(CR_GN_ALL)){
      
      print(paste("Running for Priority: ",CR_PR_ALL[p,1], "Running for Solution Area: ",CR_SA_ALL[s,1]," s=",s," Group Name: ",CR_GN_ALL[g,1]," g=",g,sep=""))
      
      RT_OVERALL_l<- res_time_prep[which(res_time_prep$Group_Assigned == CR_GN_ALL[g,1] & res_time_prep$Priority == CR_PR_ALL[p,1] & res_time_prep$Solution_Area== CR_SA_ALL[s,1]),]
      RT_OVERALL_l_mean <- round(mean(RT_OVERALL_l$res_time),2)
      
      mean_t <- c(RT_OVERALL_l_mean)
      df_mean_t <- data.frame(mean_t)
      colnames(df_mean_t)[1] <- "Mean_res_time"
      df_mean_t$Sol_Area <- CR_SA_ALL[s,1]
      df_mean_t$Group <- CR_GN_ALL[g,1]
      df_mean_t$Prioity <- CR_PR_ALL[p,1]
      
      if (p==1 & s==1 & g==1){m_obp_bpt <- df_mean_t}
      else {m_obp_bpt <- rbind(m_obp_bpt,df_mean_t)}
      m_obp_bpt <- unique(m_obp_bpt)
      
      print(m_obp_bpt)
      
    }
  }
  
}
m_obp_bpt <- m_obp_bpt[c(2,3,1,4)]
write.csv(m_obp_bpt,"Output_median_resolution_time_IBM Bucket.csv",row.names=FALSE)

######################################################################
#####  End of IBM Bucket  Res Time  ##################################
#####                          #######################################  
######################################################################

###############################################################
##############################################################
#######
#######      Start of Overall Resolve Time
#######
##############################################################
##############################################################


#library("readxl")
#library(sqldf)
#library(lubridate)

# Read input file:
RT_OVERALL = read.csv("Resolution.csv", header = TRUE, sep="," )
str(RT_OVERALL) # Structure of dataframe

#Change format of dates
RT_OVERALL$CREATION_DAT = as.Date(format(strptime(RT_OVERALL$CREATION_DAT, format = "%d-%b-%y"), "%m/%d/%y"), format = "%m/%d/%y") 
RT_OVERALL$LAST_RESOLVED_DAT <- as.Date(format(strptime(RT_OVERALL$LAST_RESOLVED_DAT, format = "%d-%b-%y"), "%m/%d/%y"), format = "%m/%d/%y") 
RT_OVERALL$DURATION <- RT_OVERALL$LAST_RESOLVED_DAT-RT_OVERALL$CREATION_DAT # Incident duration is difference of creation date and last resolve date in days
RT_OVERALL <- RT_OVERALL[!(is.na(RT_OVERALL$DURATION)),]  # drop null values
RT_OVERALL <- RT_OVERALL[which(RT_OVERALL$DURATION < 10),] # Exclude all incidents more than 10 days
RT_OVERALL$SOL_AREA[RT_OVERALL$SOL_AREA == 'OTH'] = 'OBP' # Aug 6, 2019 - added the line to convert 'OTH' to 'OBP'

str(RT_OVERALL$GROUP_NAME)
str(CR_GN_ALL$Group_Name)

#levels
levels(CR_GN_ALL[g,])
levels(RT_OVERALL$GROUP_NAME)

# drop unnecessary levels in factors
CR_GN_ALL$Group_Name <- factor(CR_GN_ALL$Group_Name)
CR_GN_ALL$Group_Name 
RT_OVERALL$GROUP_NAME <- factor(RT_OVERALL$GROUP_NAME)
levels(RT_OVERALL$GROUP_NAME)
str(RT_OVERALL$GROUP_NAME)
levels(RT_OVERALL$GROUP_NAME)[levels(RT_OVERALL$GROUP_NAME)=="IBM-ECOM-ISOM_Reporting-SL2"] <- "IBM-CMB-BRS-SL2" 

CR_SA_ALL$Solution_Area <- factor(CR_SA_ALL$Solution_Area)
CR_SA_ALL$Solution_Area

RT_OVERALL$SOL_AREA <- factor(RT_OVERALL$SOL_AREA)
levels(RT_OVERALL$SOL_AREA)
levels(RT_OVERALL$SOL_AREA)[levels(RT_OVERALL$SOL_AREA)==""] <- "OBP" 
unique(RT_OVERALL$SOL_AREA)

# Unique priority
RT_OVERALL <- RT_OVERALL[which(RT_OVERALL$INC_PRIORITY != 	"Critical"),]
CR_PR_ALL <- sqldf('select distinct INC_PRIORITY from RT_OVERALL')

#drop unnecessary levels
levels(CR_PR_ALL$INC_PRIORITY)
CR_PR_ALL$INC_PRIORITY <- factor(CR_PR_ALL$INC_PRIORITY)
levels(CR_PR_ALL$INC_PRIORITY)
RT_OVERALL$INC_PRIORITY <- factor(RT_OVERALL$INC_PRIORITY)

# for loop to calculate mean resolution time
for (p in 1:nrow(CR_PR_ALL)){
  for (s in 1:nrow(CR_SA_ALL)){
    for (g in 1:nrow(CR_GN_ALL)){
      
      print(paste("Running for Priority: ",CR_PR_ALL[p,1], "Running for Solution Area: ",CR_SA_ALL[s,1]," s=",s," Group Name: ",CR_GN_ALL[g,1]," g=",g,sep=""))
      
      RT_OVERALL_l<- RT_OVERALL[which(RT_OVERALL$GROUP_NAME == CR_GN_ALL[g,1] & RT_OVERALL$INC_PRIORITY == CR_PR_ALL[p,1] & RT_OVERALL$SOL_AREA== CR_SA_ALL[s,1]),]
      RT_OVERALL_l_mean <- round(mean(RT_OVERALL_l$DURATION),2)
      
      mean_t <- c(RT_OVERALL_l_mean)
      df_mean_t <- data.frame(mean_t)
      colnames(df_mean_t)[1] <- "Mean_res_time"
      df_mean_t$Sol_Area <- CR_SA_ALL[s,1]
      df_mean_t$Group <- CR_GN_ALL[g,1]
      df_mean_t$Prioity <- CR_PR_ALL[p,1]
      
      if (p==1 & s==1 & g==1){m_obp_bpt <- df_mean_t}
      else {m_obp_bpt <- rbind(m_obp_bpt,df_mean_t)}
      m_obp_bpt <- unique(m_obp_bpt)
      
      print(m_obp_bpt)
      
    }
  }
  
}
m_obp_bpt <- m_obp_bpt[c(2,3,1,4)]
write.csv(m_obp_bpt,"Output_median_resolution_time_Overall.csv",row.names=FALSE)


##############################################################
#######
#######      End of Overall Resolve Time
#######
##############################################################


###############################################################
##############################################################
#######
#######      End of Resolve Time
#######
##############################################################
##############################################################
